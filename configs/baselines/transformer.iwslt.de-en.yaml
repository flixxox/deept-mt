# Pipeline Arguments
qsub_mem_train:   '16G'
qsub_time_train:  '48:00:00'
qsub_mem_search:  '8G'
qsub_time_search: '48:00:00'

data_train_root:  '/home/fschmidt/data/iwslt/de-en/webdataset'
data_train_mask:  'train.tar'

data_dev_root:    '/home/fschmidt/data/iwslt/de-en/webdataset'
data_dev_mask:    'dev.tar'
corpus_size_dev:  6750

dev_src_comet:    '/home/fschmidt/data/iwslt/de-en/comet/dev.de.detok' # DeepT workflow
dev_ref:          '/home/fschmidt/data/iwslt/de-en/ref/dev.en.detok' # DeepT workflow

vocab_src:        '/home/fschmidt/data/iwslt/de-en/vocabs/source.vocab.pad.pkl'
vocab_tgt:        '/home/fschmidt/data/iwslt/de-en/vocabs/target.vocab.pad.pkl'

data_decoding:      'text'
data_preprocess:    'mt_preprocess'
data_collate:       'mt_collate'
data_len_fn:        'mt_tgt_len_fn'
postprocessing_fn:  'mt_postprocess'

search_test_set:          False
batch_size:               8192    # [target tokens without padding]
batch_size_search:        100     # [num sentences]
update_freq:              2
min_sample_size:          3
max_sample_size:          128

dataloader_workers:                   4
buffer_size_bucketing:                50000
buffer_size_batch_shuffling:          1000
buffer_size_shuffle_before_batching:  150000

buffer_sort_search: 1000

mixed_precision_training: False
quantize_post_training:   False
quantize_backend:         'x86'  # ['x86', 'qnnpack']

number_of_gpus:   1
seed:             80420
deterministic:    True

checkpoints:            True
checkpoint_unit:        'Step'  # ['Step', 'Epoch']
checkpoint_strategy:    'All'   # ['All', 'Best']
checkpoint_period:      300
checkpoint_start_after: 3000
units_to_train:         30000

average_last_after_best_checkpoints:  True
average_last_checkpoints:             True
checkpoints_to_average:               30
best_checkpoint_indicator:            'ppl'

model:        'Transformer'
model_input:  ['src', 'tgt']
encL:         6
decL:         6
model_dim:    512
ff_dim:       1024
dropout:      0.3
nHeads:       8
tiew:         True

initializer:              'GlorotUniform'
variance_scaling_scale:   0.78

criterion:            'LabelSmoothingCrossEntropy'
scores:               ['CrossEntropy']
label_smoothing:      0.1

optimizer:        'Adam'
lr_scheduler:     'Warmup'
warmup_steps:     4000
warmup_lr_scale:  2.0

search_algorithm:   'MTBeamSearch'
beam_search_input:  ['src']
beam_size:          12
length_norm:        True
length_penalty:     0.0
stepwise:           True

search_print_per_step_keys: ['src', 'result', out]