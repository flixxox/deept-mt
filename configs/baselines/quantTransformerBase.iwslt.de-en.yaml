# Pipeline Arguments
qsub_mem_train:   '16G'
qsub_time_train:  '48:00:00'
qsub_mem_search:  '8G'
qsub_time_search: '48:00:00'

data_train_root:  '/home/fschmidt/data/iwslt/de-en/webdataset'
data_train_mask:  'train.tar'

data_dev_root:    '/home/fschmidt/data/iwslt/de-en/webdataset'
data_dev_mask:    'dev.tar'
corpus_size_dev:  6750

dev_src_comet:    '/home/fschmidt/data/iwslt/de-en/comet/dev.de.detok' # DeepT workflow
dev_ref:          '/home/fschmidt/data/iwslt/de-en/ref/dev.en.detok' # DeepT workflow

vocab_src:        '/home/fschmidt/data/iwslt/de-en/vocabs/source.vocab.pad.pkl'
vocab_tgt:        '/home/fschmidt/data/iwslt/de-en/vocabs/target.vocab.pad.pkl'

data_decoding:      'text'
data_preprocess:    'mt_preprocess'
data_collate:       'mt_collate'
data_len_fn:        'mt_tgt_len_fn'
postprocessing_fn:  'mt_postprocess'

search_test_set:          False
batch_size:               1024    # [target tokens without padding]
batch_size_search:        100     # [num sentences]
update_freq:              16
min_sample_size:          3
max_sample_size:          128

dataloader_workers:                   4
buffer_size_bucketing:                50000
buffer_size_batch_shuffling:          1000
buffer_size_shuffle_before_batching:  150000

buffer_sort_search: 1000

# ------ Quantization Params

mixed_precision_training: False

quantize_post_training:   False
post_training_quant_type: 'calibration'
data_calib_root:          '/home/fschmidt/data/iwslt/de-en/webdataset'
data_calib_mask:          'train.tar'
quant_calibration_steps:    30 # times batch size = num sentences

# Loading weight is not used in PQT
load_weights: True
load_weights_from: '/nas/models/neurosys/output/baselines/transformer.iwslt.de-en/middle/output/checkpoints/ckpt-avg-last.pt'

quantize_backend:         'x86'  # ['x86', 'qnnpack']
model_uses_qat:           False

# MinMax observer
weight_quant_dtype:   'qint8' # ['qint8', 'quint8']
weight_quant_method:  'per_tensor' # ['per_tensor']

# MovingAverageMinMax observer
activation_quant_dtype:   'qint8' # ['qint8', 'quint8']
activation_quant_method:  'per_tensor' # ['per_tensor', 'per_channel']

# MovingAverageMinMax observer
dot_quant_dtype:   'qint8' # ['qint8', 'quint8']
dot_quant_method:  'per_tensor' # ['per_tensor', 'per_channel']

# MovingAverageMinMax observer
Av_quant_dtype:   'qint8' # ['qint8', 'quint8']
Av_quant_method:  'per_tensor' # ['per_tensor', 'per_channel']

# Remark: bits >= 64 => no quant
bits_others: 8
# Bits for attention 
bits_Wq:    8
bits_Wk:    8
bits_Wv:    8
bits_dot:   3
bits_Av:    3
bits_Wo:    8

# ------

number_of_gpus:   1
seed:             80420
deterministic:    False

checkpoints:                True
checkpoint_unit:            'Step'  # ['Step', 'Epoch']
checkpoint_strategy:        'All'   # ['All', 'Best']
checkpoint_period:          300
checkpoint_start_after:     1 # Start directly
units_to_train:             30000
checkpoint_strict_loading:  False

average_last_after_best_checkpoints:  True
average_last_checkpoints:             True
checkpoints_to_average:               30
best_checkpoint_indicator:            'ppl'

# BASE
model:        'Transformer'
model_input:  ['src', 'tgt']
encL:         6
decL:         6
model_dim:    512
ff_dim:       1024
dropout:      0.3
nHeads:       8
tiew:         True

initializer:              'GlorotUniform'
variance_scaling_scale:   0.78

criterion:            'LabelSmoothingCrossEntropy'
scores:               ['CrossEntropy']
label_smoothing:      0.1

optimizer:        'Adam'
lr_scheduler:     'Warmup'
warmup_steps:     4000
warmup_lr_scale:  2.0

search_algorithm:   'MTBeamSearch'
beam_search_input:  ['src']
beam_size:          12
length_norm:        True
length_penalty:     0.0
stepwise:           True

search_print_per_step_keys: ['src', 'result', out]